# Higgs Boson CP State 

> Original repository: https://github.com/klasocha/HiggsCP/tree/tyerniyazov-rhorho_refactored/

*DNN trained pseudo-observables for measuring the Higgs boson CP state mixing 
angle the in H → ττ decay at LHC.*

---

Here is the Quick Start guide for testing the whole ML pipeline by running some 
of the commands step by step. The whole project is designed to be open source 
and configurable. You are supposed to have the needed libraries installed 
in order to run the code, make some new changes and experiment on your own.


## Installation 

Requirements: Python 3.11.3, Git (tested on version 2.38.0).

The most forward way to install all the dependencies is to create a virtual 
environment and then install all the libraries mentioned in `requirements.txt`. 
Check the full installation guide in `documentation.pdf`, 
*Configuring The Environment*.


## Data Preparation 

The ML Flow expects data to be stored on a remote server which URL is specified
int `config.py`. Raw data should be located in the directory following specific
naming convention. Depending on what type of data is available at the moment,
you can use one of the commands below.

1. Downloading raw data and preparing it for preprocessing:
```shell
$ python main.py --action "download_and_prepare_original" --input "data"
```
This will download raw data generated by using Monte Carlo simulation methods 
and then parse and group the kinematic vectors together with weights belonging 
to the appropriate hypotheses. All the files will be in this case available in 
`data/`.

2. Alternatively, you can download grouped data which is already available on 
the server and contain a panoply of the parsed kinematic vectors and weights,
and then preprocess it:
```shell
$ python main.py --action "download_prepared_and_preprocess" --input "data" --features "Variant-All" --num_classes "51"
```
Please notice that the above examples invoke the preprocessing step for the 
given number of classes defining the discretisation level. In that case, we 
chose `51`. You can also experiment by trying an arbitraty odd number of classes 
in range `[3; )`. See details in `documentation.pdf`

3. Preprocessing data downloaded and prepared by using the first command 
mentioned above:
```shell
$ python main.py --action "preprocess" --input "data" --features Variant-All --num_classes "51"
```


## Model Training

To train the model you should specify its configuration, level of discretisation
stated as the number of classes, the number of epochs, as well as the 
input/output paths. Let's take a look at the following example:

```shell
$ python main.py --action "train" --input "data" --num_classes "51" --epochs 25 --training_method "soft_weights" --model_location "results/soft_weights/51_classes_variant_all" --features Variant-All
```
This command will load the data sets stored in `data/` and then start training 
the model over 25 epochs. The results will be stored in 
`results/soft_weights/model_1/`. The output path, of course, corresponds to the 
model configuration.


## Making Predictions

You can use the pre-trained model weights to make predictions. The following
example shows how to get predictions on training and validation data for
a model which has been previously saved in 
`results/soft_weights/51_classes_variant_all`:

```shell
$ python main.py --action "predict_train_and_valid" --input "data" --num_classes "51" --model_location "51_classes_variant_all"
```

The files containing predictions should appear in 
`results/soft_weights/51_classes_variant_all/predictions`.

---
Last update: 14 July 2024